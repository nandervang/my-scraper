# Web Scraper Application Specification

## Application Overview
A comprehensive, AI-powered web scraping platform with Apple-inspired design that monitors product prices across multiple e-commerce websites, providing users with advanced job scheduling, execution history, real-time analytics, and automated alerts through API integrations.

## Core Features

### 1. Advanced Job Management System
**User Story**: As a user, I want to create, schedule, monitor, and analyze AI-powered scraping jobs with comprehensive execution tracking.

**Acceptance Criteria**:
- Users can create scraping jobs with AI-powered configuration assistance
- Flexible scheduling system (manual, hourly, daily, weekly, monthly, custom intervals)
- Real-time job execution monitoring with status updates
- Comprehensive execution history with performance metrics
- Job templates and bulk operations for efficiency
- Advanced error handling with detailed diagnostics

**Technical Requirements**:
- JobScheduleModal component with Apple-inspired design
- JobHistoryModal with execution analytics and performance tracking
- Real-time job status updates using Supabase subscriptions
- Job queue management with priority and retry mechanisms
- Automated job scheduling with timezone support

### 2. Intelligent Website Management System
**User Story**: As a user, I want to manage a list of websites that serve as primary sources for product scraping.

**Acceptance Criteria**:
- Users can add, edit, and remove websites from their scraping list
- Each website entry includes URL, category tags, and scraping configuration
- System validates website accessibility and scraping feasibility
- Support for authentication-required websites (login credentials storage)
- Automatic discovery of similar/related websites to expand search scope

**Technical Requirements**:
- Website validation service with robot.txt compliance checking
- Configurable scraping rules per website (selectors, rate limits)
- Encrypted credential storage for authenticated sites

### 2. Product Item Management
**User Story**: As a user, I want to define specific products/items that the scraper should monitor across all configured websites.

**Acceptance Criteria**:
- Users can add products with detailed specifications (name, brand, model, keywords)
- Support for product variants (size, color, condition)
- Image recognition for product matching accuracy
- Manual product URL addition for specific listings
- Bulk import functionality for multiple products

**Technical Requirements**:
- Product matching algorithm using fuzzy string matching
- Image comparison service for visual product identification
- Product categorization and tagging system

### 3. Price Monitoring & Analytics
**User Story**: As a user, I want to track price history, identify trends, and receive recommendations on optimal purchase timing.

**Acceptance Criteria**:
- Real-time price tracking with configurable check intervals
- Historical price data visualization with trend analysis
- Price drop alerts and notifications
- "Buy now" vs "wait" recommendations based on trend analysis
- Price comparison across different websites for the same product

**Technical Requirements**:
- Time-series database for price history storage
- Statistical analysis algorithms for trend prediction
- Configurable alerting system with multiple notification channels

### 4. Enhanced Dashboard & Real-time Analytics
**User Story**: As a user, I want a comprehensive dashboard with real-time updates, performance metrics, and intelligent insights about my scraping operations.

**Acceptance Criteria**:
- Real-time dashboard with live statistics and job monitoring
- Performance metrics including success rates, execution times, and throughput
- Activity feed showing recent job executions and status changes
- Quick action buttons for common operations
- Advanced filtering and search capabilities
- Beautiful, responsive design with Apple-inspired aesthetics
- Auto-refresh functionality with configurable intervals

**Technical Requirements**:
- DashboardOverview component with real-time data updates
- Real-time statistics using Supabase subscriptions
- Performance analytics with trend analysis
- Interactive charts and visualizations
- Responsive design optimized for all device sizes

### 6. Advanced User Experience & Design System
**User Story**: As a user, I want a beautiful, intuitive interface that provides smooth interactions and comprehensive feedback.

**Acceptance Criteria**:
- Apple-inspired design language with consistent styling
- Comprehensive loading states and smooth animations
- Enhanced theme system with seamless dark/light mode switching
- Advanced error handling with user-friendly messages
- Responsive design that works perfectly on all devices
- Accessible interface following WCAG guidelines

**Technical Requirements**:
- Apple-style button components with proper focus states
- Comprehensive loading component library (spinners, cards, dots)
- Enhanced ThemeToggle with visual descriptions and smooth transitions
- CSS animations for better user feedback
- Proper ARIA labels and keyboard navigation support

### 7. API Endpoints for Automation
**User Story**: As a power user, I want API endpoints to integrate with n8n and other automation tools.

**Acceptance Criteria**:
- RESTful API for all core functionalities
- Webhook support for job status and price change notifications
- Bulk operations for managing jobs, products and websites
- API rate limiting and authentication
- Comprehensive API documentation

**API Endpoints**:
```
GET    /api/jobs                  - List all scraping jobs
POST   /api/jobs                  - Create new scraping job
PUT    /api/jobs/:id              - Update job configuration
DELETE /api/jobs/:id              - Delete scraping job
POST   /api/jobs/:id/run          - Execute job manually
GET    /api/jobs/:id/history      - Get job execution history
POST   /api/jobs/:id/schedule     - Update job schedule
GET    /api/products              - List all tracked products
POST   /api/products              - Add new product to track
PUT    /api/products/:id          - Update product details
DELETE /api/products/:id          - Remove product from tracking
GET    /api/products/:id/prices   - Get price history for product
GET    /api/websites              - List configured websites
POST   /api/websites              - Add new website
POST   /api/webhooks              - Configure webhook notifications
GET    /api/alerts                - Get current alerts/recommendations
GET    /api/analytics/dashboard   - Get dashboard statistics
```
**User Story**: As a power user, I want API endpoints to integrate with n8n and other automation tools.

**Acceptance Criteria**:
- RESTful API for all core functionalities
- Webhook support for price change notifications
- Bulk operations for managing products and websites
- API rate limiting and authentication
- Comprehensive API documentation

**API Endpoints**:
```
GET    /api/products              - List all tracked products
POST   /api/products              - Add new product to track
PUT    /api/products/:id          - Update product details
DELETE /api/products/:id          - Remove product from tracking
GET    /api/products/:id/prices   - Get price history for product
GET    /api/websites              - List configured websites
POST   /api/websites              - Add new website
POST   /api/webhooks              - Configure webhook notifications
GET    /api/alerts                - Get current alerts/recommendations
```

## Technical Architecture

### Frontend Stack
- **Framework**: React 18 with TypeScript
- **Build Tool**: Vite for fast development and optimized builds
- **UI Components**: shadcn/ui with Apple-inspired design system
- **State Management**: React Query for server state, Zustand for client state
- **Routing**: React Router for SPA navigation
- **Charts**: Recharts for analytics and price trend visualizations
- **Theme System**: Enhanced dark/light mode with seamless transitions
- **Animation**: CSS animations and transitions for better UX

### Backend Services
- **Database**: Supabase (PostgreSQL) with real-time subscriptions
- **Authentication**: Supabase Auth with row-level security
- **Storage**: Supabase Storage for product images and scraping screenshots
- **Edge Functions**: Supabase Edge Functions with Google AI SDK integration

### AI-Powered Scraping Workflow
1. **Website Analysis**: Gemini analyzes website structure and identifies product pages
2. **Element Detection**: Vision model identifies product images, prices, and relevant data
3. **Intelligent Navigation**: AI agent navigates through pagination, filters, and search results
4. **Product Matching**: Multimodal comparison between user-defined products and found items
5. **Price Extraction**: Context-aware price detection handling various formats and currencies
6. **Validation**: AI validates extracted data for accuracy and flags anomalies
7. **Learning**: System learns from successful extractions to improve future performance

### External Integrations
- **AI Services**: Google Gemini Pro Vision for intelligent web navigation and multimodal analysis
- **Web Automation**: Playwright with Gemini-powered element selection and reasoning
- **Proxy Services**: Rotating proxy service with AI-managed rotation patterns
- **Image Processing**: Gemini Vision for product image recognition and comparison
- **Notification Services**: Email and webhook notifications
- **AI Infrastructure**: Supabase Edge Functions with Google AI SDK integration

### Deployment Architecture
- **Frontend Hosting**: Netlify with automatic deployments from Git
- **Database**: Supabase managed PostgreSQL
- **CDN**: Netlify CDN for static assets
- **Environment Management**: Separate staging and production environments

## Data Models

### Enhanced Scraping Jobs Table
```sql
scraping_jobs (
  id: uuid PRIMARY KEY,
  user_id: uuid REFERENCES auth.users,
  name: text NOT NULL,
  url: text NOT NULL,
  scraping_type: text NOT NULL,
  prompt: text,
  status: text DEFAULT 'idle',
  gemini_model: text DEFAULT 'models/gemini-pro-vision',
  use_vision: boolean DEFAULT false,
  schedule_config: jsonb,
  last_run: timestamp,
  next_run: timestamp,
  execution_count: integer DEFAULT 0,
  success_count: integer DEFAULT 0,
  avg_execution_time: interval,
  created_at: timestamp,
  updated_at: timestamp
)
```

### Job Execution History Table
```sql
scraping_job_executions (
  id: uuid PRIMARY KEY,
  job_id: uuid REFERENCES scraping_jobs,
  status: text NOT NULL,
  started_at: timestamp NOT NULL,
  completed_at: timestamp,
  duration: interval,
  items_scraped: integer DEFAULT 0,
  error_message: text,
  ai_insights: jsonb,
  results_preview: text[],
  gemini_reasoning: text,
  screenshot_urls: text[]
)
```

### Products Table
```sql
scraper_products (
  id: uuid PRIMARY KEY,
  user_id: uuid REFERENCES auth.users,
  name: text NOT NULL,
  brand: text,
  model: text,
  keywords: text[],
  category: text,
  image_url: text,
  target_price: decimal,
  created_at: timestamp,
  updated_at: timestamp
)
```

### Websites Table
```sql
scraper_websites (
  id: uuid PRIMARY KEY,
  user_id: uuid REFERENCES auth.users,
  name: text NOT NULL,
  base_url: text NOT NULL,
  scraping_rules: jsonb,
  is_active: boolean DEFAULT true,
  created_at: timestamp
)
```

### Price History Table
```sql
scraper_price_history (
  id: uuid PRIMARY KEY,
  product_id: uuid REFERENCES scraper_products,
  website_id: uuid REFERENCES scraper_websites,
  price: decimal NOT NULL,
  currency: text DEFAULT 'USD',
  availability: boolean,
  product_url: text,
  scraped_at: timestamp,
  ai_confidence: decimal,
  gemini_reasoning: text
)
```

### AI Scraping Sessions Table
```sql
scraper_ai_sessions (
  id: uuid PRIMARY KEY,
  user_id: uuid REFERENCES auth.users,
  website_id: uuid REFERENCES scraper_websites,
  status: text DEFAULT 'running',
  gemini_model_used: text,
  products_found: integer DEFAULT 0,
  prices_extracted: integer DEFAULT 0,
  ai_insights: jsonb,
  screenshot_urls: text[],
  error_log: text,
  started_at: timestamp DEFAULT NOW(),
  completed_at: timestamp
)
```

## Performance Requirements
- **Scraping Frequency**: Configurable intervals (1-24 hours)
- **Response Time**: Dashboard loads in < 2 seconds
- **Concurrent Users**: Support 100+ concurrent users
- **Data Retention**: 2 years of price history
- **Uptime**: 99.5% availability target

## Security Considerations
- **Data Encryption**: All sensitive data encrypted at rest
- **Rate Limiting**: API and scraping rate limits to prevent abuse
- **User Isolation**: Row-level security for multi-tenant data
- **Input Validation**: All user inputs sanitized and validated
- **Monitoring**: Comprehensive logging and error tracking